{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfrFoSDE+D6K7jl6scnMSc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dlkt101101/STAT441-Project/blob/main/STAT441_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STAT 441: Statistical Learning - Classification\n",
        "## Classifying Astrophysical Images\n",
        "Prepared by:\\\n",
        "Darren Alexander Lam Kin Teng\\\n",
        "Ojus Udagani\\\n",
        "Raghuv"
      ],
      "metadata": {
        "id": "zFlh60M8q_5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install tensorflow"
      ],
      "metadata": {
        "id": "JYhXNHzor0Ho"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Wfb7tY5D1dAO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import kagglehub\n",
        "import keras\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input, Dense, Conv2D, Flatten, RandomFlip, RandomRotation, \\\n",
        "RandomZoom, RandomShear, Normalization, Dropout\n",
        "from keras.activations import relu, sigmoid, softmax\n",
        "from keras.applications import VGG16, VGG19, ResNet50\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import SparseCategoricalCrossentropy"
      ],
      "metadata": {
        "id": "BGNkJ7cgvtkL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the dataset"
      ],
      "metadata": {
        "id": "RmIhjDhCw078"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = kagglehub.dataset_download(\"engeddy/astrophysical-objects-image-dataset\")"
      ],
      "metadata": {
        "id": "UytYAuwhR7y2",
        "outputId": "cd693f69-74b5-4fdf-951b-9f9dcae02ffc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/engeddy/astrophysical-objects-image-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.41G/1.41G [00:36<00:00, 42.1MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path"
      ],
      "metadata": {
        "id": "8hx-whY2LzYA",
        "outputId": "02d5c808-aac1-4091-e756-9d3a0554a9a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/root/.cache/kagglehub/datasets/engeddy/astrophysical-objects-image-dataset/versions/1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_path = path+'/astro_dataset_maxia/astro_dataset_maxia/training'\n",
        "test_path = path+'/astro_dataset_maxia/astro_dataset_maxia/test'\n",
        "validation_path = path+'/astro_dataset_maxia/astro_dataset_maxia/validation'"
      ],
      "metadata": {
        "id": "LV2ZGTvxw0WU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating training, validation and testing batches for images."
      ],
      "metadata": {
        "id": "nXGYWKBEoc4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE =32\n",
        "IMAGE_SIZE =(256, 256)"
      ],
      "metadata": {
        "id": "7MbIqMYl7vTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_batch = tf.keras.utils.image_dataset_from_directory(\n",
        "    directory=training_path,\n",
        "    labels='inferred',\n",
        "    label_mode='int',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=IMAGE_SIZE\n",
        ")\n",
        "\n",
        "validation_batch = tf.keras.utils.image_dataset_from_directory(\n",
        "    directory=validation_path,\n",
        "    labels='inferred',\n",
        "    label_mode='int',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=IMAGE_SIZE\n",
        ")\n",
        "\n",
        "test_batch = tf.keras.utils.image_dataset_from_directory(\n",
        "    directory=test_path,\n",
        "    labels='inferred',\n",
        "    label_mode='int',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=IMAGE_SIZE\n",
        ")\n",
        "\n",
        "class_names = training_batch.class_names"
      ],
      "metadata": {
        "id": "7XkviqTvSsmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing the Images"
      ],
      "metadata": {
        "id": "zvPp-KKD8gdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in training_batch.take(1):\n",
        "    num_images = images.shape[0]\n",
        "    rows = int(num_images / 8) + 1\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    plt.suptitle(\"First Batch of Images\", fontsize=16)\n",
        "    for i in range(num_images):\n",
        "        plt.subplot(rows, 8, i + 1)\n",
        "\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "\n",
        "        # Get the class name for the title\n",
        "        label_index = labels[i].numpy()\n",
        "        title = training_batch.class_names[label_index]\n",
        "\n",
        "        plt.title(title, fontsize=8)\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()\n",
        "    break"
      ],
      "metadata": {
        "id": "G7UZ8Az795Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing image preprocessing by:\n",
        "1. Resizing the image (completed and resized to (256,256) )\n",
        "2. Normalizing the pixel values\n",
        "3. Data Augmentations\\\n",
        "  3.1. Normalization of pixel values\\\n",
        "  3.2. Random Rotations\\\n",
        "  3.3. Random Shear\\\n",
        "  3.4. Random Zoom\\\n",
        "  3.5. Random Flip"
      ],
      "metadata": {
        "id": "jY_J3FAcvNN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess = [\n",
        "    Normalization(axis=-1), # we normalize the images per channel (RGB)\n",
        "    RandomRotation(0.5), # random rotations of images\n",
        "    RandomShear(x_factor=0.9, y_factor=0.9), # Random shear of images\n",
        "    RandomZoom(0.2),\n",
        "    RandomFlip(mode=\"horizontal_and_vertical\") # Random flip of the images\n",
        "    ]"
      ],
      "metadata": {
        "id": "G8GKfBNVvM59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Implementaion"
      ],
      "metadata": {
        "id": "dy6DdSAioiqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Neural Network Implementaion"
      ],
      "metadata": {
        "id": "EKMUeLhAo0sQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will implement multiple pre-trained Convolutional Neural Networks (CNN) and fine-tune them to our astrophysical dataset. Such models include:\n",
        "* VGG16\n",
        "* VGG19\n",
        "* ResNet\n",
        "#### VGG 16"
      ],
      "metadata": {
        "id": "O8nO0YY_paFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mVGG16 = VGG16(include_top= False,\n",
        "              input_shape=(256,256,3),\n",
        "              pooling='avg',\n",
        "              weights='imagenet',\n",
        "              name = \"vgg16\")\n",
        "\n",
        "# we will avoid training on previous layers\n",
        "for layer in mVGG16.layers:\n",
        "  layer.trainable = False"
      ],
      "metadata": {
        "id": "10k248IQo9aB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mVGG16.summary()"
      ],
      "metadata": {
        "id": "-dXvjwsABj6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([Input(shape=(256,256,3), batch_size=BATCH_SIZE)] + preprocess + \\\n",
        " [mVGG16,\n",
        "  Dropout(0.5),\n",
        "  Dense(256, activation='softmax'),\n",
        "  Dense(len(class_names), activation='softmax')\n",
        "  ])\n",
        "model.compile(loss=SparseCategoricalCrossentropy(from_logits=False), optimizer = Adam(learning_rate=0.001))"
      ],
      "metadata": {
        "id": "frqBYUPbqMFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x=training_batch, validation_data=validation_batch, epochs=5, verbose=2, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "-hW0-t1AtCdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trees Implementation"
      ],
      "metadata": {
        "id": "6_2TmQhyov9r"
      }
    }
  ]
}